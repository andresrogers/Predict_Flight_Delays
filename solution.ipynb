{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to Predict Delays in Flights for SCL Airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import holidays\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "# import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load dataset\n",
    "Create DataFrame, check data and format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column details for file `dataset_SCL.csv`:\n",
    "\n",
    "- **Fecha-I** : Date and time of the programmed flight.\n",
    "- **Vlo-I** : Programmed flight number.\n",
    "- **Ori-I** : City code of programmed origin.\n",
    "- **Des-I** : City code of programmed destination.\n",
    "- **Emp-I** : Airline code of programmed flight.\n",
    "- **Fecha-O** : Date and time of operated flight.\n",
    "- **Vlo-O** : Operated flight number.\n",
    "- **Ori-O** : City code of operated origin.\n",
    "- **Des-O** : City code of operated destination.\n",
    "- **Emp-O** : Airline code of operated flight.\n",
    "- **DIA** : Day number of operated flight.\n",
    "- **MES** : Month number of operated flight.\n",
    "- **AÃ‘O** : Year of operated flight.\n",
    "- **DIANOM** : Day of the week of operated flight.\n",
    "- **TIPOVUELO** : Type of flight, I =International, N =National.\n",
    "- **OPERA** : Name of the operated airline.\n",
    "- **SIGLAORI** : Origin city name.\n",
    "- **SIGLADES** : Destination city name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset as Panda's DataFrame\n",
    "\n",
    "dataset = pd.read_csv(\"input/dataset_SCL.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data shape as: (rows, columns)\n",
    "\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated rows\n",
    "\n",
    "print(f\"Are there duplicates?: {len(dataset.drop_duplicates()) != len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for missing values (NaN's)\n",
    "\n",
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's only 1 missing value from one column, corresponding to an empy value for the column `Vlo-I : Operated flight number`. \n",
    "\n",
    "To avoid removing the whole row of data, we will analyze the frequency in which `Vl-O` is equal to `Vlo-I`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How often is 'Vlo-I' equal to 'Vlo-O' ?\n",
    "\n",
    "fraction = (dataset[\"Vlo-I\"] == dataset[\"Vlo-O\"]).sum() / len(dataset)\n",
    "\n",
    "print(f\"'Vlo-I' is equal to 'Vlo-O' {fraction:.1%} of the time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a very high probability (over 97%) that 'Vlo-O' can be correctly estimated from 'Vlo-I', so we decide to fill this value instead of removing the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missinf values for 'Vlo-O' from column 'Vlo-I'\n",
    "dataset[\"Vlo-O\"].fillna(dataset[\"Vlo-I\"], inplace=True)\n",
    "# count NaN's\n",
    "dataset.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check min-max values for Day, Month and Year\n",
    "\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show unique values for each column to better undestand the data\n",
    "\n",
    "print(\"Unique values for column 'Ori-I':\", set(dataset[\"Ori-I\"]), \"\\n\")\n",
    "print(\"Unique values for column 'Des-I':\", set(dataset[\"Des-I\"]), \"\\n\")\n",
    "print(\"Unique values for column 'Emp-I':\", set(dataset[\"Emp-I\"]), \"\\n\")\n",
    "print(\"Unique values for column 'Ori-O':\", set(dataset[\"Ori-O\"]), \"\\n\")\n",
    "print(\"Unique values for column 'Des-O':\", set(dataset[\"Des-O\"]), \"\\n\")\n",
    "print(\"Unique values for column 'Emp-O':\", set(dataset[\"Emp-O\"]), \"\\n\")\n",
    "print(\"Unique values for column 'DIANOM':\", set(dataset[\"DIANOM\"]), \"\\n\")\n",
    "print(\"Unique values for column 'TIPOVUELO':\", set(dataset[\"TIPOVUELO\"]), \"\\n\")\n",
    "print(\"Unique values for column 'OPERA':\", set(dataset[\"OPERA\"]), \"\\n\")\n",
    "print(\"Unique values for column 'SIGLAORI':\", set(dataset[\"SIGLAORI\"]), \"\\n\")\n",
    "print(\"Unique values for column 'SIGLADES':\", set(dataset[\"SIGLADES\"]), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform dates from string into datetime object to calculate time difference\n",
    "dataset[\"Fecha-I\"] = dataset[\"Fecha-I\"].apply(\n",
    "    lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n",
    ")\n",
    "dataset[\"Fecha-O\"] = dataset[\"Fecha-O\"].apply(\n",
    "    lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analyze Distribution of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Flights vs Day of the Week\n",
    "\n",
    "g = sns.catplot(\n",
    "    x=\"DIANOM\",\n",
    "    data=dataset,\n",
    "    kind=\"count\",\n",
    "    order=[\"Lunes\", \"Martes\", \"Miercoles\", \"Jueves\", \"Viernes\", \"Sabado\", \"Domingo\"],\n",
    ")\n",
    "g.set_axis_labels(\"\\nDay of the Week\\n\", \"\\nTotal Flights\\n\", size=16)\n",
    "g.fig.suptitle(\"\\nNumber of Flights per Day of the Week\", size=20, y=1.15)\n",
    "g.figure.set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Flights vs Month\n",
    "\n",
    "g = sns.catplot(x=\"MES\", data=dataset, kind=\"count\", order=range(1, 13))\n",
    "g.set_axis_labels(\"\\nMonth\\n\", \"\\nTotal Flights\\n\", size=16)\n",
    "g.fig.suptitle(\"\\nNumber of Flights per Month\", size=20, y=1.15)\n",
    "g.figure.set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Flights vs Type of Flight\n",
    "\n",
    "g = sns.catplot(x=\"TIPOVUELO\", data=dataset, kind=\"count\")\n",
    "g.set_axis_labels(\n",
    "    \"\\nType of Flight (International or National)\\n\", \"\\nTotal Flights\\n\", size=16\n",
    ")\n",
    "g.fig.suptitle(\"\\nNumber of Flights per Type of Flight\", size=20, y=1.15)\n",
    "g.figure.set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percent of National and International Flights\n",
    "\n",
    "national_pct = dataset[dataset[\"TIPOVUELO\"] == \"N\"][\"TIPOVUELO\"].count() / len(dataset)\n",
    "\n",
    "print(f\"{national_pct:.1%} of Flights are National\")\n",
    "print(f\"{1-national_pct:.1%} of Flights are International\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Flights per Operating Airline\n",
    "\n",
    "g = sns.catplot(\n",
    "    x=\"OPERA\", data=dataset, kind=\"count\", order=dataset[\"OPERA\"].value_counts().index\n",
    ")\n",
    "g.set_axis_labels(\"\\nOperating Airline\\n\", \"\\nTotal Flights\\n\", size=16)\n",
    "g.fig.suptitle(\"\\nNumber of Flights per Operating Airline\", size=20, y=1.15)\n",
    "g.figure.set_size_inches(15, 5)\n",
    "# rotate xlabels\n",
    "g.set_xticklabels(rotation=90)\n",
    "for ax in g.axes.flat:\n",
    "    ax.yaxis.set_major_formatter(lambda x, p: f\"{x:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percent for Airline 'Grupo LATAM' and 'Sky Airline\n",
    "total_latam = dataset[dataset[\"OPERA\"] == \"Grupo LATAM\"][\"OPERA\"].count()\n",
    "total_sky = dataset[dataset[\"OPERA\"] == \"Sky Airline\"][\"OPERA\"].count()\n",
    "\n",
    "print(f\"'Grupo LATAM' operates {total_latam/len(dataset):.1%} of the flights.\")\n",
    "print(f\"'Sky Airline' operates {total_sky/len(dataset):.1%} of the flights.\")\n",
    "print(\n",
    "    f\"Together they account for {(total_sky+total_latam)/len(dataset):.1%} of flights.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Flights per City of Destination\n",
    "\n",
    "g = sns.catplot(\n",
    "    x=\"SIGLADES\",\n",
    "    data=dataset,\n",
    "    kind=\"count\",\n",
    "    order=dataset[\"SIGLADES\"].value_counts().index,\n",
    ")\n",
    "g.set_axis_labels(\"\\nCity of Destination\\n\", \"\\nTotal Flights\\n\", size=16)\n",
    "g.fig.suptitle(\"\\nNumber of Flights per City of Destination\", size=20, y=1.15)\n",
    "g.figure.set_size_inches(15, 5)\n",
    "# rotate xlabels\n",
    "g.set_xticklabels(rotation=90)\n",
    "for ax in g.axes.flat:\n",
    "    ax.yaxis.set_major_formatter(lambda x, p: f\"{x:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Main Conclusions and Interpretation:\n",
    "\n",
    "1. On average, Saturday (**Sabado**) is the day with the least flights of the whole week. The rest days are similar but slightly higher for Monday (**Lunes**), Thursday (**Jueves**) and Friday (**Viernes**).\n",
    "2. There is a difference in number of flights per month of the year, there are months with low demand (April/4, May/5 and June/6) and high demand (July/7, October/10, November/11, December/12 and January/1).\n",
    "3. Slightly more than half (54.2%) of the flights are of '**National**' Type, while the rest (45.8%) are '**International**'.\n",
    "4. Most flights are operated by '**Grupo LATAM**' (60%), followed by '**Sky Airline**' (21%). Together they account for 80.9% of total flights.\n",
    "5. As seen previously (*not plotted*), the only City of Origin is '**Santiago**' de Chile.\n",
    "6. The 5 most frequent destinations overall are (in descending order): '*Buenos Aires*', '*Antofagasta*', '*Lima*', '*Calama*' and '*Puerto Montt*'.\t\n",
    "7. The 5 most frequent **International** destinations are (in descending order): '*Buenos Aires*', '*Lima*', '*Sao Paulo*', '*Ciudad de Panama*' and '*Mendoza*'.\n",
    "8. The 5 most frequent **National** destinations are (in descending order): '*Antofagasta*', '*Calama*', '*Puerto Montt*', '*ConcepciÃ³n*' e '*Iquique*'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create additional synthetic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new synthetic features from the original data\n",
    "\n",
    "synthetic_features = pd.DataFrame(\n",
    "    [], columns=[\"temporada_alta\", \"dif_min\", \"atraso_15\", \"periodo_dia\"]\n",
    ")\n",
    "synthetic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new feature 'dif_min': difference in minutes between 'Fecha-O' and 'Fecha-I'.\n",
    "\n",
    "synthetic_features[\"dif_min\"] = dataset[\"Fecha-O\"] - dataset[\"Fecha-I\"]\n",
    "synthetic_features[\"dif_min\"] = synthetic_features[\"dif_min\"].apply(\n",
    "    lambda x: float(x.total_seconds() / 60.0)\n",
    ")\n",
    "synthetic_features[\"dif_min\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new feature 'atraso_15': 1 if dif_min > 15, 0 if not.\n",
    "\n",
    "synthetic_features[\"atraso_15\"] = synthetic_features[\"dif_min\"].apply(\n",
    "    lambda x: 1 if x > 15 else 0\n",
    ")\n",
    "synthetic_features[\"atraso_15\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new feature 'temporada_alta': 1 if 'Fecha-I' is between 15-Dic and 3-Mar,\n",
    "# or 15-Jul and 31-Jul, or 11-Sep and 30-Sep, 0 if not.\n",
    "\n",
    "synthetic_features[\"temporada_alta\"] = dataset[\"Fecha-I\"].apply(\n",
    "    lambda x: 1\n",
    "    if (x > datetime(month=1, day=1, year=2017))\n",
    "    and (x < datetime(month=3, day=3, year=2017))\n",
    "    or (x > datetime(month=12, day=15, year=2017))\n",
    "    and (x < datetime(month=3, day=3, year=2018))\n",
    "    or (x > datetime(month=7, day=15, year=2017))\n",
    "    and (x < datetime(month=7, day=31, year=2017))\n",
    "    or (x > datetime(month=9, day=11, year=2017))\n",
    "    and (x < datetime(month=9, day=30, year=2017))\n",
    "    else 0\n",
    ")\n",
    "\n",
    "temporada_alta_pct = synthetic_features[\"temporada_alta\"].sum() / len(dataset)\n",
    "print(f\"'temporada_alta' is true for {temporada_alta_pct:.1%} of the flights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new feature 'periodo_dia': 'maÃ±ana' (if time between 5:00-11:59),\n",
    "# 'tarde' (between 12:00-18:59) y 'noche' ( between 19:00-4:59), using 'Fecha-I'.\n",
    "\n",
    "synthetic_features[\"periodo_dia\"] = dataset[\"Fecha-I\"].apply(\n",
    "    lambda x: \"maÃ±ana\"\n",
    "    if (x.hour >= 5) and (x.hour < 12)\n",
    "    else \"tarde\"\n",
    "    if (x.hour >= 12) and (x.hour < 19)\n",
    "    else \"noche\"\n",
    ")\n",
    "synthetic_features[\"periodo_dia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Synthetic Features to CSV file\n",
    "\n",
    "synthetic_features.to_csv(\"output/synthetic_features.csv\", index=False)\n",
    "synthetic_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Delay Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join synthetic features into main dataframe\n",
    "\n",
    "dataset = dataset.join(synthetic_features)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Average Delay per City of Destination\n",
    "\n",
    "grouped_df = (\n",
    "    dataset.groupby(by=\"SIGLADES\", as_index=False)[\"dif_min\"]\n",
    "    .mean()\n",
    "    .sort_values(by=\"dif_min\", ascending=False)\n",
    ")\n",
    "\n",
    "g = sns.catplot(x=\"SIGLADES\", y=\"dif_min\", data=grouped_df, kind=\"bar\")\n",
    "g.set_axis_labels(\"\\nCity of Destination\\n\", \"\\nAverage Delay in minutes\\n\", size=16)\n",
    "g.fig.suptitle(\"\\n Average Delay of Flights per City of Destination\", size=20, y=1.15)\n",
    "g.figure.set_size_inches(15, 5)\n",
    "# rotate xlabels\n",
    "g.set_xticklabels(rotation=90)\n",
    "for ax in g.axes.flat:\n",
    "    ax.yaxis.set_major_formatter(lambda x, p: f\"{x:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destinations with average delay > 15\n",
    "\n",
    "grouped_df[grouped_df[\"dif_min\"] > 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 destinations with less delay\n",
    "\n",
    "grouped_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Average Delay per operating Airline\n",
    "\n",
    "grouped_df = (\n",
    "    dataset.groupby(by=\"OPERA\", as_index=False)[\"dif_min\"]\n",
    "    .mean()\n",
    "    .sort_values(by=\"dif_min\", ascending=False)\n",
    ")\n",
    "\n",
    "g = sns.catplot(x=\"OPERA\", y=\"dif_min\", data=grouped_df, kind=\"bar\")\n",
    "g.set_axis_labels(\"\\nOperating Airline\\n\", \"\\nAverage Delay in minutes\\n\", size=16)\n",
    "g.fig.suptitle(\"\\n Average Delay of Flights per Operating Airline\", size=20, y=1.15)\n",
    "g.figure.set_size_inches(15, 5)\n",
    "# rotate xlabels\n",
    "g.set_xticklabels(rotation=90)\n",
    "for ax in g.axes.flat:\n",
    "    ax.yaxis.set_major_formatter(lambda x, p: f\"{x:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airlines with average delay > 15\n",
    "\n",
    "grouped_df[grouped_df[\"dif_min\"] > 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 Airlines with less delay\n",
    "\n",
    "grouped_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Average Delay per MONTH\n",
    "\n",
    "grouped_df = (\n",
    "    dataset.groupby(by=\"MES\", as_index=False)[\"dif_min\"]\n",
    "    .mean()\n",
    "    .sort_values(by=\"dif_min\", ascending=False)\n",
    ")\n",
    "\n",
    "g = sns.catplot(x=\"MES\", y=\"dif_min\", data=grouped_df, kind=\"bar\")\n",
    "g.set_axis_labels(\"\\nMONTH\\n\", \"\\nAverage Delay in minutes\\n\", size=16)\n",
    "g.fig.suptitle(\"\\n Average Delay of Flights per MONTH\", size=20, y=1.15)\n",
    "g.figure.set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dÃ­a de la semana\n",
    "# Distribution of Average Delay per Day of the Week\n",
    "\n",
    "grouped_df = dataset.groupby(by=\"DIANOM\", as_index=False)[\"dif_min\"].mean()\n",
    "\n",
    "g = sns.catplot(\n",
    "    x=\"DIANOM\",\n",
    "    y=\"dif_min\",\n",
    "    data=grouped_df,\n",
    "    kind=\"bar\",\n",
    "    order=[\"Lunes\", \"Martes\", \"Miercoles\", \"Jueves\", \"Viernes\", \"Sabado\", \"Domingo\"],\n",
    ")\n",
    "g.set_axis_labels(\"\\nDay of the Week\\n\", \"\\nAverage Delay in minutes\\n\", size=16)\n",
    "g.fig.suptitle(\"\\n Average Delay of Flights per Day of the Week\", size=20, y=1.15)\n",
    "g.figure.set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between Average Delay vs Season\n",
    "\n",
    "dataset[[\"temporada_alta\", \"dif_min\"]].corr(method=\"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Average Delay per Season\n",
    "\n",
    "grouped_df = (\n",
    "    dataset.groupby(by=\"temporada_alta\", as_index=False)[\"dif_min\"]\n",
    "    .mean()\n",
    "    .sort_values(by=\"dif_min\", ascending=False)\n",
    ")\n",
    "\n",
    "g = sns.catplot(x=\"temporada_alta\", y=\"dif_min\", data=grouped_df, kind=\"bar\")\n",
    "g.set_axis_labels(\"\\nHigh season\\n\", \"\\nAverage Delay in minutes\\n\", size=16)\n",
    "g.fig.suptitle(\"\\n Average Delay of Flights per Season (Low or High)\", size=20, y=1.15)\n",
    "g.figure.set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df[\"dif_min\"][1] - grouped_df[\"dif_min\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between Average Delay vs Flight Type\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"TIPOVUELO\": dataset[\"TIPOVUELO\"].replace({\"I\": 1, \"N\": 0}),\n",
    "        \"dif_min\": dataset[\"dif_min\"],\n",
    "    }\n",
    ").corr(method=\"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Average Delay per Flight Type\n",
    "\n",
    "grouped_df = (\n",
    "    dataset.groupby(by=\"TIPOVUELO\", as_index=False)[\"dif_min\"]\n",
    "    .mean()\n",
    "    .sort_values(by=\"dif_min\", ascending=False)\n",
    ")\n",
    "\n",
    "g = sns.catplot(x=\"TIPOVUELO\", y=\"dif_min\", data=grouped_df, kind=\"bar\")\n",
    "g.set_axis_labels(\n",
    "    \"\\nFlight Type (International or National)\\n\",\n",
    "    \"\\nAverage Delay in minutes\\n\",\n",
    "    size=16,\n",
    ")\n",
    "g.fig.suptitle(\"\\n Average Delay per Flight Type\", size=20, y=1.15)\n",
    "g.figure.set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df[\"dif_min\"][0] - grouped_df[\"dif_min\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Average Delay per period of day\n",
    "\n",
    "grouped_df = dataset.groupby(by=\"periodo_dia\", as_index=False)[\"dif_min\"].mean()\n",
    "\n",
    "g = sns.catplot(\n",
    "    x=\"periodo_dia\",\n",
    "    y=\"dif_min\",\n",
    "    data=grouped_df,\n",
    "    kind=\"bar\",\n",
    "    order=[\"maÃ±ana\", \"tarde\", \"noche\"],\n",
    ")\n",
    "g.set_axis_labels(\"\\nPeriod of day\\n\", \"\\nAverage Delay in minutes\\n\", size=16)\n",
    "g.fig.suptitle(\"\\n Average Delay of Flights per Period of day\", size=20, y=1.15)\n",
    "g.figure.set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Main Conclusions and Interpretation:\n",
    "\n",
    "1. There are 11 destinations that are more than 15 minutes late on average, they are: 'Cochabamba', 'Quito', 'Puerto Stanley', 'Sydney', 'Ushuia', 'Melbourne', 'Rosario', 'Bariloche', 'Auckland N.Z.', 'Toronto' and 'Mendoza'.\n",
    "2. The average delay for 'Cochabamba', 'Quito' and 'Puerto Stanley' is over 50 minutes.\n",
    "3. The 10 less delayed destinatios are: 'Ciudad de Mexico', 'Pisco, Peru', 'Paris', 'Cataratas Iguacu', 'Ciudad de Panama', 'Atlanta', 'Washington', 'Dallas', 'Houston', 'Curitiba, Bra.'.\n",
    "4. The average delay for 'Ciudad de Panama', 'Atlanta' and 'Washington' is less than 1 minute.\n",
    "5. 'Dallas', 'Houston' and 'Curitiba, Bra.' depart more than 1 minute early on average.\n",
    "6. The Airlines with more than 15 minutes of average delay are: 'Plus Ultra Lineas Aereas', 'Qantas Airways', 'Latin American Wings', 'Air Canada'.\n",
    "7. The 10 less delayed Airlines are: 'Alitalia', 'Lacsa', 'Iberia', 'Air France', 'K.L.M.', 'American Airlines', 'Copa Air', 'Delta Air', 'United Airlines', 'Aeromexico'.\n",
    "8. 'Air France', 'K.L.M.', 'American Airlines', 'Copa Air' and 'Delta Air' are delayed less than 2 minutes on average.\n",
    "9. 'United Airlines' and 'Aeromexico' depart around 2 minutes early on average.\n",
    "10. The most delayed Months are July/7, October/10 and December/12, with July having an average delay of more than 15 minutes.\n",
    "11. The week days with highest average delay are Monday (Lunes) and Friday (Viernes). The least delays occur on Sunday (Domingo).\n",
    "12. There is a very little difference of 0.82 minutes of additional delay on average between High and Low season\n",
    "13. There is a positive but low 'spearman' correlation of 0.0275 between minutes of delay and high_season. This means that there is a little effect of season on delay but is not a strong feature on its own to predict delay. The Spearman correlation is used over the typical Pearson correlation because it assesses monotonic relationships (whether linear or not linear).\n",
    "14. There is a little difference of 2.82 minutes of additional delay on average between the Type of Flight (International or National).\n",
    "15. There is a positive but low 'spearman' correlation of 0.056 between minutes of delay and Type of Flight. This means that there is a little effect of season on delay but is not a strong feature on its own to predict delay.\n",
    "16. The average delay on the morning flights is around 1.7 minutes less than on the afternoon and night."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 What would be the most important features to predict delays? \n",
    "\n",
    "According to the previous analysis and observations, the best features would be those that showed a strong variation between the feature categories and the delay in minutes.\n",
    "\n",
    "Features with big influence:\n",
    "- City of Destination ('SIGLADES')\n",
    "- Operating Airline ('OPERA)\n",
    "- Month ('MES')\n",
    "- Day of Week ('DIANOM')\n",
    "\n",
    "Features with medium influence:\n",
    "- Flight Type ('TIPOVUELO')\n",
    "- Period of day ('periodo_dia')\n",
    "\n",
    "Features with small influence:\n",
    "- High season ('temporada_alta'): did not show so much strength, but using the exact dates could introduce a better signal for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Machine Learning Model(s)\n",
    "- The main goal is to estimate the probability of delay for a flight.\n",
    "- Delay is assumed as a binary class with 1 meaning that the flight departed with > 15min delay.\n",
    "- The Model must be a binary classifier and transform score into probability\n",
    "- Optional: create another regression model to predict the exact delay in minutes. => minutes of delay could also be transformed into delay probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Include Additional Features\n",
    "\n",
    "- did a change occur to the flight program ? (between programmed and operation data) => **Is this data known a priori?**\n",
    "- use date as 'mes-dia' categorical feature, ignore year (careful not to over-fit)\n",
    "- include list of holidays for chile, non-working days, school vacations, etc.\n",
    "- include meteorological data for SCL Airport (Arturo Merino Benitez)\n",
    "- Try various ML and Deep Neural Network models.\n",
    "- Optimize hyperparameters to increase performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did the program change ?\n",
    "\n",
    "dataset[\"Vlo-I\"] != dataset[\"Vlo-O\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Average Delay per specific date (MONTH-DAY)\n",
    "\n",
    "# create Month-Day feature\n",
    "dataset[\"MES-DIA\"] = dataset[\"MES\"].astype(str) + \"-\" + dataset[\"DIA\"].astype(str)\n",
    "\n",
    "grouped_df = (\n",
    "    dataset.groupby(by=\"MES-DIA\", as_index=False)[\"dif_min\"]\n",
    "    .mean()\n",
    "    .sort_values(by=\"dif_min\", ascending=False)\n",
    ")\n",
    "\n",
    "g = sns.catplot(\n",
    "    x=\"MES-DIA\", y=\"dif_min\", data=grouped_df[grouped_df[\"dif_min\"] > 15], kind=\"bar\"\n",
    ")\n",
    "g.set_axis_labels(\n",
    "    \"\\nSpecific date (MONTH-DAY)\\n\", \"\\nAverage Delay in minutes\\n\", size=16\n",
    ")\n",
    "g.fig.suptitle(\n",
    "    \"\\n Average Delay (> 15 minutes) per specific date (MONTH-DAY)\", size=20, y=1.15\n",
    ")\n",
    "g.figure.set_size_inches(15, 5)\n",
    "# rotate xlabels\n",
    "g.set_xticklabels(rotation=90)\n",
    "for ax in g.axes.flat:\n",
    "    ax.yaxis.set_major_formatter(lambda x, p: f\"{x:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Delay seems to be more associated with specific date combinations than simply using the High/Low season periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metereological Data\n",
    "According to a [report](https://www.transportation.gov/sites/dot.gov/files/docs/kulesa_Weather_Aviation.pdf) from the US Federal Aviation Administration (FAA), weather conditions cause ~70% of the delays in the US National Airspace System.\n",
    "\n",
    "Because metereological conditions are very important variables that could cause delays, data for SCL Airport was downloaded from https://climatologia.meteochile.gob.cl/ and included in the `input/additional_data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative Humidity per Hour\n",
    "humidity = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\n",
    "            \"input/additional_data/330021_2017_Humedad_.csv.zip\",\n",
    "            compression=\"zip\",\n",
    "            sep=\";|,\",\n",
    "        ),\n",
    "        pd.read_csv(\n",
    "            \"input/additional_data/330021_2018_Humedad_.csv.zip\",\n",
    "            compression=\"zip\",\n",
    "            sep=\";|,\",\n",
    "        ),\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "humidity.rename(\n",
    "    columns={\"momento\": \"date\", \"HR_Valor\": \"relative_humidity\"}, inplace=True\n",
    ")\n",
    "humidity[\"date\"] = humidity[\"date\"].apply(\n",
    "    lambda x: datetime.strptime(x, \"%d-%m-%Y %H:%M:%S\")\n",
    ")\n",
    "humidity.set_index(\"date\", inplace=True)\n",
    "humidity[\"relative_humidity\"] = humidity[\"relative_humidity\"].apply(lambda x: x / 100)\n",
    "humidity = humidity.interpolate(method=\"linear\")\n",
    "humidity.drop(columns=[\"CodigoNacional\"], inplace=True)\n",
    "humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dew point (the temperature in which air becomes saturated with water vapor)\n",
    "\n",
    "dew_point = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\n",
    "            \"input/additional_data/330021_2017_PuntoRocio_.csv.zip\",\n",
    "            compression=\"zip\",\n",
    "            sep=\";|,\",\n",
    "        ),\n",
    "        pd.read_csv(\n",
    "            \"input/additional_data/330021_2018_PuntoRocio_.csv.zip\",\n",
    "            compression=\"zip\",\n",
    "            sep=\";|,\",\n",
    "        ),\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "dew_point.rename(columns={\"momento\": \"date\", \"Td_Valor\": \"dew_point\"}, inplace=True)\n",
    "dew_point[\"date\"] = dew_point[\"date\"].apply(\n",
    "    lambda x: datetime.strptime(x, \"%d-%m-%Y %H:%M:%S\")\n",
    ")\n",
    "dew_point.set_index(\"date\", inplace=True)\n",
    "dew_point = dew_point.interpolate(method=\"time\")\n",
    "dew_point.drop(columns=[\"CodigoNacional\"], inplace=True)\n",
    "dew_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Air Temperature per hour\n",
    "\n",
    "temp = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\n",
    "            \"input/additional_data/330021_2017_Temperatura_.csv.zip\",\n",
    "            compression=\"zip\",\n",
    "            sep=\";|,\",\n",
    "        ),\n",
    "        pd.read_csv(\n",
    "            \"input/additional_data/330021_2018_Temperatura_.csv.zip\",\n",
    "            compression=\"zip\",\n",
    "            sep=\";|,\",\n",
    "        ),\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "temp.rename(columns={\"momento\": \"date\", \"Ts_Valor\": \"temperature\"}, inplace=True)\n",
    "temp[\"date\"] = temp[\"date\"].apply(lambda x: datetime.strptime(x, \"%d-%m-%Y %H:%M:%S\"))\n",
    "temp.set_index(\"date\", inplace=True)\n",
    "temp = temp.interpolate(method=\"time\")\n",
    "temp.drop(columns=[\"CodigoNacional\"], inplace=True)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative precipitation per Hour (6-hr interval interpolated to 1-hr interval)\n",
    "precipitation = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\n",
    "            \"input/additional_data/330021_2017_Agua6Horas_.csv.zip\",\n",
    "            compression=\"zip\",\n",
    "            sep=\";|,\",\n",
    "        ),\n",
    "        pd.read_csv(\n",
    "            \"input/additional_data/330021_2018_Agua6Horas_.csv.zip\",\n",
    "            compression=\"zip\",\n",
    "            sep=\";|,\",\n",
    "        ),\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "precipitation.rename(columns={\"momento\": \"date\", \"RRR6_Valor\": \"rain_mm\"}, inplace=True)\n",
    "precipitation[\"date\"] = precipitation[\"date\"].apply(\n",
    "    lambda x: datetime.strptime(x, \"%d-%m-%Y %H:%M:%S\")\n",
    ")\n",
    "precipitation.set_index(\"date\", inplace=True)\n",
    "# reindex interval from 6 hours to 1 hour difference\n",
    "precipitation = precipitation.reindex(\n",
    "    pd.date_range(\"2017-01-01 00:00:00\", \"2018-12-31 23:00:00\", freq=\"H\")\n",
    ")\n",
    "# interpolate missing values then normalize to maintain the same total mm of rain\n",
    "precipitation = precipitation.interpolate(method=\"time\").apply(lambda x: x / 6)\n",
    "precipitation.drop(columns=[\"CodigoNacional\", \"Traza_Valor\"], inplace=True)\n",
    "precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presence of Fog:\n",
    "\n",
    "If the temperature of the air is around 2.5ÂºC of the dew point temperature, then the water vapor condensed will remain suspended in the air in the form of tiny droplets of water forming a fog. Also if Relative Humidty > 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formation of Fog per Hour\n",
    "\n",
    "fog = [\n",
    "    1\n",
    "    if abs(temp.loc[date, \"temperature\"] - dew_point.loc[date, \"dew_point\"]) <= 2.5\n",
    "    or humidity.loc[date, \"relative_humidity\"] >= 0.95\n",
    "    else 0\n",
    "    for date in temp.index\n",
    "]\n",
    "fog = pd.DataFrame(fog, index=temp.index, columns=[\"fog\"])\n",
    "fog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presence of Ice and Frost:\n",
    "\n",
    "When temperature is below 2ÂºC as measured on air, the ground is close to or under 0ÂºC which forms ice (cold air descends).\n",
    "\n",
    "If it's raining then there will be ice formed on the runway.\n",
    "\n",
    "Also, if the air temperature is below the dew point, then the condensed water vapor will form dew that will turn into frost on the runway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formation of Frost per Hour\n",
    "frost = [\n",
    "    1\n",
    "    if temp.loc[date, \"temperature\"] <= 2\n",
    "    and (temp.loc[date, \"temperature\"] < dew_point.loc[date, \"dew_point\"])\n",
    "    or precipitation.loc[date, \"rain_mm\"] > 0\n",
    "    else 0\n",
    "    for date in temp.index\n",
    "]\n",
    "frost = pd.DataFrame(frost, index=temp.index, columns=[\"frost\"])\n",
    "frost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Coverage per Day (resample to hour, interpolating data)\n",
    "cloud_coverage = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\n",
    "            \"input/additional_data/330021_2017_CieloCubierto_.csv.zip\",\n",
    "            compression=\"zip\",\n",
    "            sep=\",\",\n",
    "        ),\n",
    "        pd.read_csv(\n",
    "            \"input/additional_data/330021_2018_CieloCubierto_.csv.zip\",\n",
    "            compression=\"zip\",\n",
    "            sep=\",\",\n",
    "        ),\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "cloud_coverage[\"Cloud Coverage %\"] = cloud_coverage[\"Octavas de Cielo Cubierto\"].apply(\n",
    "    lambda x: x / 8\n",
    ")\n",
    "cloud_coverage.rename(columns={\"Fecha\": \"date\"}, inplace=True)\n",
    "cloud_coverage[\"date\"] = cloud_coverage[\"date\"].apply(\n",
    "    lambda x: datetime.strptime(x, \"%Y-%m-%d\")\n",
    ")\n",
    "cloud_coverage.set_index(\"date\", inplace=True)\n",
    "# change values to 1 hour, interpolating missing values\n",
    "cloud_coverage = cloud_coverage.reindex(\n",
    "    pd.date_range(\"2017-01-01 00:00:00\", \"2018-12-31 23:00:00\", freq=\"H\")\n",
    ")\n",
    "cloud_coverage = cloud_coverage.interpolate(method=\"time\")\n",
    "cloud_coverage.drop(columns=\"Octavas de Cielo Cubierto\", inplace=True)\n",
    "cloud_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wind per Hour\n",
    "wind = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\n",
    "            \"input/additional_data/330021_2017_Viento_.csv.zip\",\n",
    "            compression=\"zip\",\n",
    "            sep=\";|,\",\n",
    "        ),\n",
    "        pd.read_csv(\n",
    "            \"input/additional_data/330021_2018_Viento_.csv.zip\",\n",
    "            compression=\"zip\",\n",
    "            sep=\";|,\",\n",
    "        ),\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "wind.rename(columns={\"momento\": \"date\", \"ff_Valor\": \"wind_speed\"}, inplace=True)\n",
    "wind[\"date\"] = wind[\"date\"].apply(lambda x: datetime.strptime(x, \"%d-%m-%Y %H:%M:%S\"))\n",
    "wind.set_index(\"date\", inplace=True)\n",
    "wind = wind.interpolate(method=\"time\")\n",
    "wind.drop(columns=[\"CodigoNacional\", \"dd_Valor\", \"VRB_Valor\"], inplace=True)\n",
    "wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atmospheric pressure\n",
    "atm_pressure = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\n",
    "            \"input/additional_data/330021_2017_PresionQFE_.csv.zip\",\n",
    "            compression=\"zip\",\n",
    "            sep=\";|,\",\n",
    "        ),\n",
    "        pd.read_csv(\n",
    "            \"input/additional_data/330021_2018_PresionQFE_.csv.zip\",\n",
    "            compression=\"zip\",\n",
    "            sep=\";|,\",\n",
    "        ),\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "atm_pressure.rename(columns={\"momento\": \"date\", \"QFE_Valor\": \"pressure\"}, inplace=True)\n",
    "atm_pressure[\"date\"] = atm_pressure[\"date\"].apply(\n",
    "    lambda x: datetime.strptime(x, \"%d-%m-%Y %H:%M:%S\")\n",
    ")\n",
    "atm_pressure.set_index(\"date\", inplace=True)\n",
    "atm_pressure = atm_pressure.interpolate(method=\"time\")\n",
    "atm_pressure.drop(columns=[\"CodigoNacional\"], inplace=True)\n",
    "atm_pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Meteorological data into one dataframe\n",
    "\n",
    "weather = pd.concat(\n",
    "    [cloud_coverage, atm_pressure, fog, frost, wind, temp, precipitation, humidity],\n",
    "    axis=1,\n",
    ")\n",
    "# fill NaN's with previous value\n",
    "weather.fillna(method=\"ffill\", inplace=True)\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there still NaN's ?\n",
    "\n",
    "weather.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Holiday data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[[\"Fecha-I\", \"AÃ‘O\", \"MES\", \"DIA\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Holidays applicable to Chile\n",
    "\n",
    "CL_holidays = holidays.Chile()\n",
    "# list comprehension to mark holidays as 1 and non-holidays as 0\n",
    "is_holiday = [1 if date in CL_holidays else 0 for date in dataset[\"Fecha-I\"]]\n",
    "# add column to dataset\n",
    "dataset[\"is_holiday\"] = is_holiday\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between Average Delay vs Holidays\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"is_holiday\": dataset[\"is_holiday\"],\n",
    "        \"dif_min\": dataset[\"dif_min\"],\n",
    "    }\n",
    ").corr(method=\"spearman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delay doesn't seem to be correlated to holidays (it even shows a weak negative corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add School Vacation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new feature 'temporada_alta': 1 if 'Fecha-I' is between 15-Dic and 3-Mar,\n",
    "# or 15-Jul and 31-Jul, or 11-Sep and 30-Sep, 0 if not.\n",
    "\n",
    "dataset[\"is_school_vacation\"] = dataset[\"Fecha-I\"].apply(\n",
    "    lambda x: 1\n",
    "    if (x >= datetime(month=1, day=1, year=2017))\n",
    "    and (x <= datetime(month=3, day=6, year=2017))\n",
    "    or (x >= datetime(month=7, day=10, year=2017))\n",
    "    and (x <= datetime(month=7, day=21, year=2017))\n",
    "    or (x >= datetime(month=9, day=17, year=2017))\n",
    "    and (x <= datetime(month=9, day=21, year=2017))\n",
    "    or (x >= datetime(month=12, day=13, year=2017))\n",
    "    else 0\n",
    ")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between Average Delay vs School Vacation\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"is_school_vacation\": dataset[\"is_school_vacation\"],\n",
    "        \"dif_min\": dataset[\"dif_min\"],\n",
    "    }\n",
    ").corr(method=\"spearman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's only a weak positive correlation between School Vacations and Delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  is the dataset balanced ?\n",
    "g = sns.catplot(x=\"atraso_15\", data=dataset, kind=\"count\")\n",
    "g.set_axis_labels(\n",
    "    \"\\nDelay > 15 min\\n\",\n",
    "    \"\\nTotal number of Flights\\n\",\n",
    "    size=16,\n",
    ")\n",
    "g.fig.suptitle(\"\\nDelay > 15 minutes per Total number of Flights\", size=20, y=1.15)\n",
    "g.figure.set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_imbalance = (\n",
    "    dataset[dataset[\"atraso_15\"] == 0].count()[0]\n",
    "    / dataset[dataset[\"atraso_15\"] == 1].count()[0]\n",
    ")\n",
    "print(f\"The class imbalance ratio is: {class_imbalance:.1f} to 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a big imbalance between the binary class to predict, which should be considered when evaluating the performance of the model (If not considered, a random binary model with 50% chance per class would predict the correct class 81.5% of the time with the current imbalance ratio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6055fe7821f3d16f1658f9c73811435b69e21008c3fe00e08f70f128e2c41590"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('Predict_Flight_Delays--sOdaDLH')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
